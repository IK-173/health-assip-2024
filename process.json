[{
  "id" : "c9zwrg",
  "name" : "1. COPD data prep",
  "description" : null,
  "code" : "# Write your first Python code in Geoweaver\nimport pandas as pd\nimport geopandas as gpd\n\n# Read CDC lung disease data\nCOPD_df = pd.read_csv('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/COPD/COPD_data.csv')\n\n# Read EPA air quality data for PM2.5 and ozone\npm25_df = pd.read_csv('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/combined_pm25_data.csv', low_memory=False)\nozone_df = pd.read_csv('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/combined_ozone_data.csv', low_memory=False)\n\n# Read county shapefile for spatial analysis\ncounties_gdf = gpd.read_file('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/County data/county_shapefile.shp')\n\n# Convert date column to datetime\npm25_df['Date Local'] = pd.to_datetime(pm25_df['Date Local'], format='%Y-%m-%d')\nozone_df['Date Local'] = pd.to_datetime(ozone_df['Date Local'], format='%Y-%m-%d')\n\n# Extract year from date\npm25_df['year'] = pm25_df['Date Local'].dt.year\nozone_df['year'] = ozone_df['Date Local'].dt.year\n\n# Get year from PM2.5\n\n# Convert 'Date Local' to datetime and extract year\npm25_df['Date Local'] = pd.to_datetime(pm25_df['Date Local'], format='%Y-%m-%d')\npm25_df['year'] = pm25_df['Date Local'].dt.year\n\n# Rename 'Sample Measurement' to 'PM2.5'\npm25_df.rename(columns={'Arithmetic Mean': 'PM2.5'}, inplace=True)\n\n# Verify the DataFrame\nprint(pm25_df.head())\nprint(pm25_df.columns)\n\n# Convert 'Date Local' to datetime and extract year\nozone_df['Date Local'] = pd.to_datetime(ozone_df['Date Local'], format='%m/%d/%y')\nozone_df['year'] = ozone_df['Date Local'].dt.year\n\n# Rename 'Sample Measurement' to 'Ozone'\nozone_df.rename(columns={'Arithmetic Mean': 'Ozone'}, inplace=True)\n\n# Verify the DataFrame\nprint(ozone_df.head())\nprint(ozone_df.columns)\n\n# Ensure 'County Name' and 'year' exist\nprint(pm25_df[['County Name', 'year']].head())\nprint(ozone_df[['County Name', 'year']].head())\n\n# Group by county and year to calculate annual averages\npm25_annual = pm25_df.groupby(['County Name', 'year'])['PM2.5'].mean().reset_index()\nozone_annual = ozone_df.groupby(['County Name', 'year'])['Ozone'].mean().reset_index()\n\n# Get latitude and longitude for each county-year pair\nlat_lon_pm25 = pm25_df.groupby(['County Name', 'year']).agg({\n    'Latitude': 'first',\n    'Longitude': 'first'\n}).reset_index()\n\nlat_lon_ozone = ozone_df.groupby(['County Name', 'year']).agg({\n    'Latitude': 'first',\n    'Longitude': 'first'\n}).reset_index()\n\n# Merge latitude and longitude with PM2.5 and Ozone averages\npm25_annual = pd.merge(pm25_annual, lat_lon_pm25, on=['County Name', 'year'])\nozone_annual = pd.merge(ozone_annual, lat_lon_ozone, on=['County Name', 'year'])\n# this worked but merging had error described below\n\n# Problems to fix: the lung disease dataframe columns are unnamed, the PM and ozone dataframe columns have 'County Name' while the lung disease doesn't\n\n# The header of the Lung disease data is 'Interstitial lung...' not the column name so...\n# Reload the lung disease dataframe with the correct header\nCOPD_df = pd.read_csv('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/COPD/COPD_data.csv', header=1)\n\n\n# Problem: lung disease data does not have 'County Name'\n# Rename columns to be consistent\nCOPD_df.rename(columns={'Location': 'County Name'}, inplace=True)\n\n# Verify the columns, all should have 'County Name' and 'year'\nprint(\"PM2.5 DataFrame columns after renaming:\")\nprint(pm25_df.columns)\nprint(\"Ozone DataFrame columns after renaming:\")\nprint(ozone_df.columns)\nprint(\"COPD DataFrame columns after renaming:\")\nprint(COPD_df.columns)\n\n# problem is 'year'\n# extract the year from the columns, \"'Mortality Rate, 1980*', 'Mortality Rate, 1985*'\"\n# Melt the DataFrame\n\n# Use the melt function to transform the DataFrame from wide to long format. This will convert the year-specific columns into rows.\nCOPD_long = COPD_df.melt(\n    id_vars=['County Name', 'FIPS', '% Change in Mortality Rate, 1980-2014'],\n    var_name='year',\n    value_name='Mortality Rate'\n)\n\n# Extract the Year\n# Extract the year from the 'Year' column using string operations and convert it to an integer.\nCOPD_long['year'] = COPD_long['year'].str.extract(r'(\\d{4})').astype(int)\n# r'(\\d{4})' is a regex pattern that matches and captures a four-digit year.\n\n# test again and see what it looks like now, make sure to print the long version (changed version)\nprint(COPD_long)\n\n# Ensure that the year column exists in all DataFrames used in the merge operation. \nprint(\"Lung Disease DataFrame columns:\")\nprint(COPD_long.columns)\n\nprint(\"PM2.5 Annual DataFrame columns:\")\nprint(pm25_annual.columns)\n\nprint(\"Ozone Annual DataFrame columns:\")\nprint(ozone_annual.columns)\n\n# Convert year columns to integer type\nCOPD_long['year'] = COPD_long['year'].astype(int)\npm25_annual['year'] = pm25_annual['year'].astype(int)\nozone_annual['year'] = ozone_annual['year'].astype(int)\n\n# Tried merging but the merged_df file contained no data\n# Problem: Lung disease data doesn't have latitude and longitutde like PM2.5 and ozone data\n# County Names are different\n\n# Check unique values in the County Name and year columns\nprint(COPD_long[['County Name', 'year']].drop_duplicates().head())\nprint(pm25_annual[['County Name', 'year']].drop_duplicates().head())\nprint(ozone_annual[['County Name', 'year']].drop_duplicates().head())\n\n# Define a function to standardize county names\ndef clean_county_name(name):\n    return name.strip().lower()\n\n# Apply the function to each DataFrame\nCOPD_long['County Name'] = COPD_long['County Name'].apply(clean_county_name)\npm25_annual['County Name'] = pm25_annual['County Name'].apply(clean_county_name)\nozone_annual['County Name'] = ozone_annual['County Name'].apply(clean_county_name)\n\n\n# Remove 'county' from the County Name in the lung disease dataset\nCOPD_long['County Name'] = COPD_long['County Name'].str.replace(' county', '', case=False)\n\n# Assuming the state and county columns are named 'State' and 'County Name'\n# Adjust column names if they are different in your datasets\n\n# Remove any extra spaces and clean up\npm25_annual['County Name'] = pm25_annual['County Name'].str.strip()\nozone_annual['County Name'] = ozone_annual['County Name'].str.strip()\n\n# Verify the new format\nprint(pm25_annual[['County Name', 'year']].drop_duplicates().sort_values(['County Name', 'year']))\nprint(ozone_annual[['County Name', 'year']].drop_duplicates().sort_values(['County Name', 'year']))\n\n\nprint(pm25_annual.head)\nprint(ozone_annual.head)\n\n# Convert county names to lowercase to standardize\npm25_annual['County Name'] = pm25_annual['County Name'].str.lower()\nozone_annual['County Name'] = ozone_annual['County Name'].str.lower()\nCOPD_long['County Name'] = COPD_long['County Name'].str.lower()\n\n# Find common counties\ncommon_counties = set(pm25_annual['County Name']).intersection(set(ozone_annual['County Name']))\n\n# Filter DataFrames by common counties\npm25_common = pm25_annual[pm25_annual['County Name'].isin(common_counties)]\nozone_common = ozone_annual[ozone_annual['County Name'].isin(common_counties)]\nCOPD_common = COPD_long[COPD_long['County Name'].isin(common_counties)]\n\n# Print results\nprint(\"PM2.5 DataFrame with common counties:\\n\", pm25_common)\nprint(\"Ozone DataFrame with common counties:\\n\", ozone_common)\nprint(\"Lung Disease DataFrame with common counties:\\n\", ozone_common)\nprint(f\"Number of common counties: {len(common_counties)}\")\n\n# Fixed county problem\n# Now I need to find common years\n\n# Find common years across the datasets\ncommon_years = set(pm25_common['year']).intersection(set(ozone_common['year'])).intersection(set(COPD_common['year']))\n\n# Filter DataFrames by common years\npm25_common = pm25_common[pm25_common['year'].isin(common_years)]\nozone_common = ozone_common[ozone_common['year'].isin(common_years)]\nCOPD_common = COPD_common[COPD_common['year'].isin(common_years)]\n\n# Verify the common counties and years\nprint(f\"Number of common counties: {len(common_counties)}\")\nprint(f\"Number of common years: {len(common_years)}\")\n\n# Check unique years in each DataFrame\nprint(\"Unique years in PM2.5 data:\", pm25_common['year'].unique())\nprint(\"Unique years in Ozone data:\", ozone_common['year'].unique())\nprint(\"Unique years in Lung Disease data:\", COPD_common['year'].unique())\n\nprint(\"Number of rows in PM2.5 data before filtering:\", len(pm25_annual))\nprint(\"Number of rows in Ozone data before filtering:\", len(ozone_annual))\nprint(\"Number of rows in Lung Disease data before filtering:\", len(COPD_long))\n\n# Find common counties again and print them\ncommon_counties = set(pm25_annual['County Name']).intersection(set(ozone_annual['County Name']))\nprint(f\"Number of common counties: {len(common_counties)}\")\nprint(f\"Common counties: {common_counties}\")\n\n# Filter DataFrames by common counties and print their lengths\npm25_common = pm25_annual[pm25_annual['County Name'].isin(common_counties)]\nozone_common = ozone_annual[ozone_annual['County Name'].isin(common_counties)]\nCOPD_common = COPD_long[COPD_long['County Name'].isin(common_counties)]\n\nprint(\"Number of rows in PM2.5 data after filtering:\", len(pm25_common))\nprint(\"Number of rows in Ozone data after filtering:\", len(ozone_common))\nprint(\"Number of rows in Lung Disease data after filtering:\", len(COPD_common))\n\nprint(\"Unique years in PM2.5 data before filtering:\", pm25_annual['year'].unique())\nprint(\"Unique years in Ozone data before filtering:\", ozone_annual['year'].unique())\nprint(\"Unique years in Lung Disease data before filtering:\", COPD_long['year'].unique())\n\n# Now county name and year columns should be fixed\n# Merge lung disease data with PM2.5 and ozone data\nnew_merged_df = pd.merge(COPD_long, pm25_annual, on=['County Name', 'year'])\nnew_merged_df = pd.merge(new_merged_df, ozone_annual, on=['County Name', 'year'])\n\nprint(new_merged_df.head)\n\nnew_merged_df.to_csv('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/new_merged_df.csv', index=False)\n\n\n# Find rows where Latitude_x and Latitude_y differ\ndiscrepancies = new_merged_df[new_merged_df['Latitude_x'] != new_merged_df['Latitude_y']]\n\n# Display the rows with discrepancies\nprint(discrepancies[['County Name', 'year', 'Latitude_x', 'Latitude_y']])\n\n# Drop the incorrect columns and rename the correct ones\nnew_merged_df = new_merged_df.drop(columns=['Latitude_y', 'Longitude_y'])\nnew_merged_df = new_merged_df.rename(columns={'Latitude_x': 'Latitude', 'Longitude_x': 'Longitude'})\n\n# Recreate the Geometry Column - Once you have cleaned up the latitude and longitude columns:\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n# Create the geometry column using the cleaned latitude and longitude columns\nnew_merged_df['geometry'] = new_merged_df.apply(lambda row: Point(row['Longitude'], row['Latitude']), axis=1)\n\n# Convert to GeoDataFrame\ngdf1 = gpd.GeoDataFrame(new_merged_df, geometry='geometry')\n\n# Optionally, set the coordinate reference system (CRS) if you know it\n# Example: gdf.set_crs(epsg=4326, inplace=True) # WGS84 CRS\n\n# Save the GeoDataFrame to a shapefile or other format if needed\ngdf1.to_file('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/new_merged_geodataframe.shp')\n\n# Save the updated DataFrame to CSV\nnew_merged_df.to_csv('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/new_merged_df.csv', index=False)\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "f2jtv8",
  "name" : "2. COPD Data Analysis",
  "description" : null,
  "code" : "import pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom esda.moran import Moran, Moran_Local\nfrom libpysal.weights import Queen\nfrom splot.esda import lisa_cluster\n\n# Load the merged dataset\nnew_merged_df = pd.read_csv('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/COPD/new_merged_df.csv')\n\n# Load the shapefile into a GeoDataFrame\ngdf1 = gpd.read_file('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/COPD/new_merged_geodataframe.shp')\n\n# Verify the loaded GeoDataFrame\nprint(gdf1.head())\nprint(gdf1.columns)\nprint(gdf1.crs)  # Check the Coordinate Reference System\n\n# Descriptive Statistics\nsummary_stats = gdf1.describe()\nprint(summary_stats)\n\n# Plot distribution of PM2.5 levels and Ozone levels\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\ngdf1['PM2.5'].hist(bins=20)\nplt.title('Distribution of PM2.5 Levels')\nplt.xlabel('PM2.5')\nplt.ylabel('Frequency')\n\nplt.subplot(1, 2, 2)\ngdf1['Ozone'].hist(bins=20)\nplt.title('Distribution of Ozone Levels')\nplt.xlabel('Ozone')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n# Scatter plot of PM2.5 vs. Mortality\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.scatter(gdf1['PM2.5'], gdf1['Mortality'], alpha=0.5, edgecolors='w', s=80)\nplt.title('PM2.5 vs. Mortality')\nplt.xlabel('PM2.5')\nplt.ylabel('Mortality')\n\n# Scatter plot of Ozone vs. Mortality\nplt.subplot(1, 2, 2)\nplt.scatter(gdf1['Ozone'], gdf1['Mortality'], alpha=0.5, edgecolors='w', s=80)\nplt.title('Ozone vs. Mortality')\nplt.xlabel('Ozone')\nplt.ylabel('Mortality')\n\nplt.tight_layout()\nplt.show()\n\n# Display sample data for verification\nprint(gdf1[['PM2.5', 'Mortality', 'Ozone']].head())\n\n# Extract the main value from 'Mortality'\ndef extract_mortality(value):\n    return value.split(' ')[0]\n\ngdf1['Mortality'] = gdf1['Mortality'].apply(extract_mortality).astype(float)\n\n# Set the CRS manually to EPSG:4326 if needed\ngdf1.crs = 'EPSG:4326'\n\n# Verify the CRS\nprint(\"CRS:\", gdf1.crs)\n\n# Create spatial weights matrix using Queen contiguity\nweights = Queen.from_dataframe(gdf1, use_index=True)\n\n# Perform Moran's I analysis\nmoran = Moran(gdf1['Mortality'], weights)\nprint(f\"Moran's I: {moran.I}, p-value: {moran.p_sim}\")\n\n# Perform Local Moran's I analysis\nmoran_local = Moran_Local(gdf1['Mortality'], weights)\n\n# Print Local Moran's I results\nprint(f\"Local Moran's I: {moran_local.Is}\")\nprint(f\"Local Moran's p-values: {moran_local.p_sim}\")\n\n# Plot Local Moran's I clusters\nlisa_cluster(moran_local, gdf1, p=0.05)\nplt.show()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "oyp46z",
  "name" : "5. COPD Regression Analysis",
  "description" : null,
  "code" : "import geopandas as gpd\nimport pandas as pd\nimport numpy as np\nfrom shapely.geometry import Point\nimport libpysal as lp\nfrom spreg import ML_Lag, ML_Error\nfrom libpysal.weights import Queen\nimport os\n\n\n# Load the shapefile into a GeoDataFrame\ngdf1 = gpd.read_file('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/new_merged_geodataframe.shp')\n\n# Load the merged dataset\nnew_merged_df = pd.read_csv('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/new_merged_df.csv')\n\n# Load the county shapefile\nshapefile_path = '/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/County data/county_shapefile.shp'\ncounty_gdf = gpd.read_file(shapefile_path)\n\n# Load the fixed_geodataframe1\nfixed_geodataframe1 = gpd.read_file('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/COPD/fixed_geodataframe1.shp')\n\nprint(new_merged_df)\nprint(fixed_geodataframe1)\n\n# Check the geometry type of merged_geodataframe\nprint(\"Geometry type of gdf:\")\nprint(gdf1.geometry.geom_type.unique())\n\n# Check the geometry type of fixed_geodataframe1\nprint(\"Geometry type of fixed_geodataframe1:\")\nprint(fixed_geodataframe1.geometry.geom_type.unique())\n\n# Check common columns\ncommon_columns = set(gdf1.columns) & set(fixed_geodataframe1.columns)\nprint(\"Common Columns:\")\nprint(common_columns)\n\n# Merge the GeoDataFrames on common columns to see differences\ncomparison_df = gdf1.merge(fixed_geodataframe1, on=list(common_columns), how='outer', indicator=True)\nprint(comparison_df.head())\n\n# Check if there are any differences\ndifferences = comparison_df[comparison_df['_merge'] != 'both']\nprint(\"Differences between the two GeoDataFrames:\")\nprint(differences)\n\n# Perform the merge operation and assign the result to gdf\ngdf1 = gdf1.merge(fixed_geodataframe1, on=list(common_columns), how='outer', indicator=True)\n\n\n# Optionally, you can drop the '_merge' column if it's no longer needed\ngdf1 = gdf1.drop(columns=['_merge'])\n\n# Now gdf contains the merged data\n\n\n# Convert 'FIPS' in gdf to string\ngdf1['FIPS'] = gdf1['FIPS'].astype(str)\n\n# Convert 'COUNTYFP' in county_gdf to string\ncounty_gdf['COUNTYFP'] = county_gdf['COUNTYFP'].astype(str)\n\nprint(gdf1['FIPS'].dtype)\nprint(county_gdf['COUNTYFP'].dtype)\n\n# Perform the merge\nnew_merged_gdf = gdf1.merge(county_gdf, left_on='FIPS', right_on='COUNTYFP', how='left')\n\nprint(new_merged_gdf.head())\nprint(new_merged_gdf.columns)\n\n# Merge was successful, now cleaning columns\n# Rename the columns to fix any typos and remove unwanted suffixes\nnew_merged_gdf = new_merged_gdf.rename(columns={'County Nam': 'County Name', '% Change i': '% Change in Mortality Rate, 1980-2014', 'geometry_x': 'geometry'})\n\n# Drop unnecessary columns (if any)\ncolumns_to_drop = ['STATEFP', 'COUNTYFP', 'COUNTYNS', 'AFFGEOID', 'GEOID', 'NAME_y', 'LSAD', 'NAME', 'geometry_y', 'ALAND', 'AWATER']\nnew_merged_gdf = new_merged_gdf.drop(columns=columns_to_drop, errors='ignore')\n\n# Convert 'Mortality' to numeric\ngdf1['Mortality'] = gdf1['Mortality'].str.extract(r'(\\d+\\.\\d+)').astype(float)\n\n# Handle NaNs\ngdf1['Mortality'] = gdf1['Mortality'].fillna(gdf1['Mortality'].mean())\n\n# Check the result\nprint(gdf1.head())\n\n# Ensure that the columns of interest are present and correctly named\nprint(new_merged_gdf.head())\nprint(new_merged_gdf.columns)\n\n# Check the data types of your DataFrame columns\nprint(gdf1[['Mortality', 'PM2.5', 'Ozone']].dtypes)\n\n# Convert to numeric, if necessary\ngdf1['Mortality'] = pd.to_numeric(gdf1['Mortality'], errors='coerce')\ngdf1['PM2.5'] = pd.to_numeric(gdf1['PM2.5'], errors='coerce')\ngdf1['Ozone'] = pd.to_numeric(gdf1['Ozone'], errors='coerce')\n\n# Calculate the mean of each column\nmeans = gdf1[['Mortality', 'PM2.5', 'Ozone']].mean()\n\n# Replace NaN values with the column mean\ngdf1[['Mortality', 'PM2.5', 'Ozone']] = gdf1[['Mortality', 'PM2.5', 'Ozone']].fillna(means)\n\n# Optionally, ensure no infinite values are present\ngdf1 = gdf1[~gdf1[['Mortality', 'PM2.5', 'Ozone']].isin([np.inf, -np.inf]).any(axis=1)]\n\n# Convert 'Mortality' to numeric, coerce errors to NaN\ngdf1['Mortality'] = pd.to_numeric(gdf1['Mortality'], errors='coerce')\n\n# Recalculate means and replace NaNs with the column mean\nmeans = gdf1[['Mortality', 'PM2.5', 'Ozone']].mean()\nprint(means)\ngdf1[['Mortality', 'PM2.5', 'Ozone']] = gdf1[['Mortality', 'PM2.5', 'Ozone']].fillna(means)\n\n# Check the data types of your DataFrame columns\nprint(gdf1[['Mortality', 'PM2.5', 'Ozone']].dtypes)\n\n# Check data types\nprint(gdf1.dtypes)\n\n# Ensure no infinite values are present\ngdf1 = gdf1[~gdf1[['Mortality', 'PM2.5', 'Ozone']].isin([np.inf, -np.inf]).any(axis=1)]\n\n# Confirm the cleaned data\nprint(gdf1.head())\n\n# Check for NaN values in your DataFrame columns\nprint(gdf1[['Mortality', 'PM2.5', 'Ozone']].isna().sum())\n\n# Confirm that there are no infinite values\nprint((np.isinf(gdf1[['Mortality', 'PM2.5', 'Ozone']])).sum())\n\n# Buffer points to create polygons\nbuffer_distance = 1  # Adjust this distance based on your analysis\ngdf1['geometry'] = gdf1['geometry'].buffer(buffer_distance)\n\n# Verify that the geometry type is now polygons\nprint(\"Geometry type after buffering:\")\nprint(gdf1.geometry.geom_type.unique())\n\n# Recreate GeoDataFrame with new polygon geometries\ngdf1 = gpd.GeoDataFrame(gdf1, geometry='geometry')\n\n# Check for NaN values in your DataFrame columns\nprint(\"NaN values in columns before processing:\")\nprint(gdf1[['Mortality', 'PM2.5', 'Ozone']].isna().sum())\n\n# Check data types and ensure no infinite values are present\nprint(\"Checking for infinite values before processing:\")\nprint(gdf1[~gdf1[['Mortality', 'PM2.5', 'Ozone']].isin([np.inf, -np.inf]).any(axis=1)])\n\n# Convert 'Mortality' to numeric\ngdf1['Mortality'] = pd.to_numeric(gdf1['Mortality'], errors='coerce')\n\n# Check for NaN values and fill with column mean\nmeans = gdf1[['Mortality', 'PM2.5', 'Ozone']].mean()\ngdf1[['Mortality', 'PM2.5', 'Ozone']] = gdf1[['Mortality', 'PM2.5', 'Ozone']].fillna(means)\n\n# Check the final data types and presence of NaN/infinite values\nprint(\"Data types of columns after processing:\")\nprint(gdf1.dtypes)\nprint(\"Number of NaN values after processing:\")\nprint(gdf1[['Mortality', 'PM2.5', 'Ozone']].isna().sum())\nprint(\"Number of infinite values after processing:\")\nprint((np.isinf(gdf1[['Mortality', 'PM2.5', 'Ozone']])).sum())\n\n# Define your dependent and independent variables\ny = gdf1[['Mortality']].values.flatten()  # Flatten to 1D array\nX = gdf1[['PM2.5', 'Ozone']].values\n\n# Ensure that the input data is valid\nprint(\"Shape of y:\", y.shape)\nprint(\"Shape of X:\", X.shape)\nprint(\"First 5 rows of y:\", y[:5])\nprint(\"First 5 rows of X:\", X[:5])\n\n# Recalculate weights matrix for polygons\ntry:\n    w = lp.weights.Queen.from_dataframe(gdf1, use_index=False)\n    w.transform = 'r'  # Row-standardize the weights\n    print(\"Row-standardized Weights matrix:\")\n    print(w.full())  # For binary weights\n\n    # Check the type of weights matrix\n    print(\"Weights matrix:\")\n    print(\"Type of weights matrix:\", type(w))\n    print(\"Weights matrix W:\")\n    print(w)\n\n    # Print the number of weights and some example values\n    print(\"Number of weights:\", len(w.weights))  # Total number of weights\n\n    # Sample output for binary weights\n    print(\"Sample weights (first 5 entries):\")\n    for key in list(w.weights.keys())[:5]:\n        print(f\"{key}: {w.weights[key]}\")\nexcept Exception as e:\n    print(\"Error in creating weights matrix:\", e)\n\n# Perform the regression with explicitly set weights matrix\ntry:\n    # Fit the ML_Lag model\n    model_lag = ML_Lag(y, X, w=w)\n    print(\"ML_Lag Model Summary:\")\n    print(model_lag.summary)\nexcept Exception as e:\n    print(\"Error in ML_Lag model:\", e)\n\ntry:\n    # Fit the ML_Error model\n    model_error = ML_Error(y, X, w=w)\n    print(\"ML_Error Model Summary:\")\n    print(model_error.summary)\nexcept Exception as e:\n    print(\"Error in ML_Error model:\", e)\n\n# Check the columns after renaming\nprint(new_merged_gdf.columns)\n\n# Ensure the CRS is set\nif new_merged_gdf.crs is None:\n    new_merged_gdf.crs = 'EPSG:4326'  # or the appropriate EPSG code for your CRS\n\n# Assuming your GeoDataFrame is named COPD_new_merged_gdf\nnew_merged_gdf.to_file('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/COPD/new_merged_gdf.gpkg', layer='new_merged_gdf', driver='GPKG')\n\nimport libpysal as ps\nfrom mgwr.sel_bw import Sel_BW\n#import mgwr \n#from mgwr.sel import Sel_BW\n\n#import mgwr \n\n#try:\n#    from mgwr.sel import Sel_BW\n #   print(\"Sel_BW imported successfully!\")\n#except ImportError as e:\n #   print(f\"ImportError: {e}\")\n\n\ncoords = list(zip(gdf1.geometry.centroid.x, gdf1.geometry.centroid.y))\ny = gdf1[['Mortality']].values\nX = gdf1[['PM2.5', 'Ozone']].values\n\n# Bandwidth selection\n#sel_bw = Sel_BW(coords, y, X,w=w)\n#bw = sel_bw.search()\n\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "dlavrw",
  "name" : "7. COPD Spatial Panel Data Models (SPDM)",
  "description" : null,
  "code" : "import pandas as pd\nimport geopandas as gpd\nfrom linearmodels.panel import PanelOLS\n\n\n# Load the GeoPackage\nnew_merged_gdf = gpd.read_file('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/COPD/new_merged_gdf.gpkg', layer='new_merged_gdf')\n\n# Convert 'Mortality' column to numeric, removing any non-numeric characters if necessary\nnew_merged_gdf['Mortality'] = pd.to_numeric(new_merged_gdf['Mortality'].str.extract('(\\d+.\\d+)')[0], errors='coerce')\n\n# Print the first few rows of the GeoDataFrame\nprint(new_merged_gdf.head())\n\n\nprint(new_merged_gdf.columns)\n\npanel_data = new_merged_gdf.set_index(['County Name', 'year'])\n\nprint(panel_data[['PM2.5', 'Ozone', 'Mortality']].isnull().sum())\n\n# Assuming panel_data is a DataFrame with 'county', 'year', and other variables\npanel_data = new_merged_gdf.set_index(['County Name', 'year'])\nexog_vars = panel_data[['PM2.5', 'Ozone']]\nendog_var = panel_data['Mortality']\n\nprint(len(exog_vars), len(endog_var))\n\nprint(panel_data[['PM2.5', 'Ozone', 'Mortality']].dtypes)\n\nprint(panel_data.head())\n\n\nsdm_model = PanelOLS(endog_var, exog_vars)\nresults = sdm_model.fit()\nprint(results.summary)",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
}]
