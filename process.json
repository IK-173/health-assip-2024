[{
  "id" : "5ps83r",
  "name" : "2. Data Analysis",
  "description" : null,
  "code" : "import pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom esda.moran import Moran, Moran_Local\nfrom libpysal.weights import Queen\nfrom splot.esda import lisa_cluster\n\n# Load the merged dataset\nmerged_df = pd.read_csv('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/Interstitial Lung Disease/merged_df.csv')\n\n# Load the shapefile into a GeoDataFrame\ngdf = gpd.read_file('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/Interstitial Lung Disease/merged_geodataframe.shp')\n\n# Verify the loaded GeoDataFrame\nprint(gdf.head())\nprint(gdf.columns)\nprint(gdf.crs)  # Check the Coordinate Reference System\n\n# Descriptive Statistics\nsummary_stats = gdf.describe()\nprint(summary_stats)\n\n# Plot distribution of PM2.5 levels and Ozone levels\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\ngdf['PM2.5'].hist(bins=20)\nplt.title('Distribution of PM2.5 Levels')\nplt.xlabel('PM2.5')\nplt.ylabel('Frequency')\n\nplt.subplot(1, 2, 2)\ngdf['Ozone'].hist(bins=20)\nplt.title('Distribution of Ozone Levels')\nplt.xlabel('Ozone')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n# Scatter plot of PM2.5 vs. Mortality\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.scatter(gdf['PM2.5'], gdf['Mortality'], alpha=0.5, edgecolors='w', s=80)\nplt.title('PM2.5 vs. Mortality')\nplt.xlabel('PM2.5')\nplt.ylabel('Mortality')\n\n# Scatter plot of Ozone vs. Mortality\nplt.subplot(1, 2, 2)\nplt.scatter(gdf['Ozone'], gdf['Mortality'], alpha=0.5, edgecolors='w', s=80)\nplt.title('Ozone vs. Mortality')\nplt.xlabel('Ozone')\nplt.ylabel('Mortality')\n\nplt.tight_layout()\nplt.show()\n\n# Display sample data for verification\nprint(gdf[['PM2.5', 'Mortality', 'Ozone']].head())\n\n# Extract the main value from 'Mortality'\ndef extract_mortality(value):\n    return value.split(' ')[0]\n\ngdf['Mortality'] = gdf['Mortality'].apply(extract_mortality).astype(float)\n\n# Set the CRS manually to EPSG:4326 if needed\ngdf.crs = 'EPSG:4326'\n\n# Verify the CRS\nprint(\"CRS:\", gdf.crs)\n\n# Create spatial weights matrix using Queen contiguity\nweights = Queen.from_dataframe(gdf, use_index=True)\n\n# Perform Moran's I analysis\nmoran = Moran(gdf['Mortality'], weights)\nprint(f\"Moran's I: {moran.I}, p-value: {moran.p_sim}\")\n\n# Perform Local Moran's I analysis\nmoran_local = Moran_Local(gdf['Mortality'], weights)\n\n# Print Local Moran's I results\nprint(f\"Local Moran's I: {moran_local.Is}\")\nprint(f\"Local Moran's p-values: {moran_local.p_sim}\")\n\n# Plot Local Moran's I clusters\nlisa_cluster(moran_local, gdf, p=0.05)\nplt.show()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "8m8qpi",
  "name" : "5. Regression Analysis",
  "description" : null,
  "code" : "import geopandas as gpd\nimport pandas as pd\nimport numpy as np\nfrom shapely.geometry import Point\nimport libpysal as lp\nfrom spreg import ML_Lag, ML_Error\nfrom libpysal.weights import Queen\nimport os\nimport random\n\nnp.random.seed(0)\nrandom.seed(0)\n\n# Load the shapefile into a GeoDataFrame\ngdf = gpd.read_file('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/Interstitial Lung Disease/merged_geodataframe.shp')\n\n# Load the merged dataset\nmerged_df = pd.read_csv('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/Interstitial Lung Disease/merged_df.csv')\n\n# Load the county shapefile\nshapefile_path = '/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/County data/county_shapefile.shp'\ncounty_gdf = gpd.read_file(shapefile_path)\n\n# Load the fixed_geodataframe1\nfixed_geodataframe1 = gpd.read_file('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/Interstitial Lung Disease/fixed_geodataframe1.shp')\n\n\n# Check the geometry type of merged_geodataframe\nprint(\"Geometry type of gdf:\")\nprint(gdf.geometry.geom_type.unique())\n\n# Check the geometry type of fixed_geodataframe1\nprint(\"Geometry type of fixed_geodataframe1:\")\nprint(fixed_geodataframe1.geometry.geom_type.unique())\n\n# Check common columns\ncommon_columns = set(gdf.columns) & set(fixed_geodataframe1.columns)\nprint(\"Common Columns:\")\nprint(common_columns)\n\n# Merge the GeoDataFrames on common columns to see differences\ncomparison_df = gdf.merge(fixed_geodataframe1, on=list(common_columns), how='outer', indicator=True)\nprint(comparison_df.head())\n\n# Check if there are any differences\ndifferences = comparison_df[comparison_df['_merge'] != 'both']\nprint(\"Differences between the two GeoDataFrames:\")\nprint(differences)\n\n# Perform the merge operation and assign the result to gdf\ngdf = gdf.merge(fixed_geodataframe1, on=list(common_columns), how='outer', indicator=True)\n\n\n# Optionally, you can drop the '_merge' column if it's no longer needed\ngdf = gdf.drop(columns=['_merge'])\n\n# Now gdf contains the merged data\n\n\n# Convert 'FIPS' in gdf to string\ngdf['FIPS'] = gdf['FIPS'].astype(str)\n\n# Convert 'COUNTYFP' in county_gdf to string\ncounty_gdf['COUNTYFP'] = county_gdf['COUNTYFP'].astype(str)\n\nprint(gdf['FIPS'].dtype)\nprint(county_gdf['COUNTYFP'].dtype)\n\n# Perform the merge\nmerged_gdf = gdf.merge(county_gdf, left_on='FIPS', right_on='COUNTYFP', how='left')\n\nprint(merged_gdf.head())\nprint(merged_gdf.columns)\n\n# Merge was successful, now cleaning columns\n# Rename the columns to fix any typos and remove unwanted suffixes\nmerged_gdf = merged_gdf.rename(columns={'County Nam': 'County Name', '% Change i': '% Change in Mortality Rate, 1980-2014', 'geometry_x': 'geometry'})\n\n# Drop unnecessary columns (if any)\ncolumns_to_drop = ['STATEFP', 'COUNTYFP', 'COUNTYNS', 'AFFGEOID', 'GEOID', 'NAME_y', 'LSAD', 'NAME', 'geometry_y', 'ALAND', 'AWATER']\nmerged_gdf = merged_gdf.drop(columns=columns_to_drop, errors='ignore')\n\n# Convert 'Mortality' to numeric\ngdf['Mortality'] = gdf['Mortality'].str.extract(r'(\\d+\\.\\d+)').astype(float)\n\n# Handle NaNs\ngdf['Mortality'] = gdf['Mortality'].fillna(gdf['Mortality'].mean())\n\n# Check the result\nprint(gdf.head())\n\n# Ensure that the columns of interest are present and correctly named\nprint(merged_gdf.head())\nprint(merged_gdf.columns)\n\n# Check the data types of your DataFrame columns\nprint(gdf[['Mortality', 'PM2.5', 'Ozone']].dtypes)\n\n# Convert to numeric, if necessary\ngdf['Mortality'] = pd.to_numeric(gdf['Mortality'], errors='coerce')\ngdf['PM2.5'] = pd.to_numeric(gdf['PM2.5'], errors='coerce')\ngdf['Ozone'] = pd.to_numeric(gdf['Ozone'], errors='coerce')\n\n# Calculate the mean of each column\nmeans = gdf[['Mortality', 'PM2.5', 'Ozone']].mean()\n\n# Replace NaN values with the column mean\ngdf[['Mortality', 'PM2.5', 'Ozone']] = gdf[['Mortality', 'PM2.5', 'Ozone']].fillna(means)\n\n# Optionally, ensure no infinite values are present\ngdf = gdf[~gdf[['Mortality', 'PM2.5', 'Ozone']].isin([np.inf, -np.inf]).any(axis=1)]\n\n# Convert 'Mortality' to numeric, coerce errors to NaN\ngdf['Mortality'] = pd.to_numeric(gdf['Mortality'], errors='coerce')\n\n# Recalculate means and replace NaNs with the column mean\nmeans = gdf[['Mortality', 'PM2.5', 'Ozone']].mean()\nprint(means)\ngdf[['Mortality', 'PM2.5', 'Ozone']] = gdf[['Mortality', 'PM2.5', 'Ozone']].fillna(means)\n\n# Check the data types of your DataFrame columns\nprint(gdf[['Mortality', 'PM2.5', 'Ozone']].dtypes)\n\n# Check data types\nprint(gdf.dtypes)\n\n# Ensure no infinite values are present\ngdf = gdf[~gdf[['Mortality', 'PM2.5', 'Ozone']].isin([np.inf, -np.inf]).any(axis=1)]\n\n# Confirm the cleaned data\nprint(gdf.head())\n\n# Check for NaN values in your DataFrame columns\nprint(gdf[['Mortality', 'PM2.5', 'Ozone']].isna().sum())\n\n# Confirm that there are no infinite values\nprint((np.isinf(gdf[['Mortality', 'PM2.5', 'Ozone']])).sum())\n\n# Buffer points to create polygons\nbuffer_distance = 1  # Adjust this distance based on your analysis\ngdf['geometry'] = gdf['geometry'].buffer(buffer_distance)\n\n# Verify that the geometry type is now polygons\nprint(\"Geometry type after buffering:\")\nprint(gdf.geometry.geom_type.unique())\n\n# Recreate GeoDataFrame with new polygon geometries\ngdf = gpd.GeoDataFrame(gdf, geometry='geometry')\n\n# Check for NaN values in your DataFrame columns\nprint(\"NaN values in columns before processing:\")\nprint(gdf[['Mortality', 'PM2.5', 'Ozone']].isna().sum())\n\n# Check data types and ensure no infinite values are present\nprint(\"Checking for infinite values before processing:\")\nprint(gdf[~gdf[['Mortality', 'PM2.5', 'Ozone']].isin([np.inf, -np.inf]).any(axis=1)])\n\n# Convert 'Mortality' to numeric\ngdf['Mortality'] = pd.to_numeric(gdf['Mortality'], errors='coerce')\n\n# Check for NaN values and fill with column mean\nmeans = gdf[['Mortality', 'PM2.5', 'Ozone']].mean()\ngdf[['Mortality', 'PM2.5', 'Ozone']] = gdf[['Mortality', 'PM2.5', 'Ozone']].fillna(means)\n\n# Check the final data types and presence of NaN/infinite values\nprint(\"Data types of columns after processing:\")\nprint(gdf.dtypes)\nprint(\"Number of NaN values after processing:\")\nprint(gdf[['Mortality', 'PM2.5', 'Ozone']].isna().sum())\nprint(\"Number of infinite values after processing:\")\nprint((np.isinf(gdf[['Mortality', 'PM2.5', 'Ozone']])).sum())\n\n# Define your dependent and independent variables\ny = gdf[['Mortality']].values.flatten()  # Flatten to 1D array\nX = gdf[['PM2.5', 'Ozone']].values\n\n# Ensure that the input data is valid\nprint(\"Shape of y:\", y.shape)\nprint(\"Shape of X:\", X.shape)\nprint(\"First 5 rows of y:\", y[:5])\nprint(\"First 5 rows of X:\", X[:5])\n\n# Recalculate weights matrix for polygons\ntry:\n    w = lp.weights.Queen.from_dataframe(gdf, use_index=False)\n    w.transform = 'r'  # Row-standardize the weights\n    print(\"Row-standardized Weights matrix:\")\n    print(w.full())  # For binary weights\n\n    # Check the type of weights matrix\n    print(\"Weights matrix:\")\n    print(\"Type of weights matrix:\", type(w))\n    print(\"Weights matrix W:\")\n    print(w)\n\n    # Print the number of weights and some example values\n    print(\"Number of weights:\", len(w.weights))  # Total number of weights\n\n    # Sample output for binary weights\n    print(\"Sample weights (first 5 entries):\")\n    for key in list(w.weights.keys())[:5]:\n        print(f\"{key}: {w.weights[key]}\")\nexcept Exception as e:\n    print(\"Error in creating weights matrix:\", e)\n\n# Perform the regression with explicitly set weights matrix\ntry:\n    # Fit the ML_Lag model\n    model_lag = ML_Lag(y, X, w=w)\n    print(\"ML_Lag Model Summary:\")\n    print(model_lag.summary)\nexcept Exception as e:\n    print(\"Error in ML_Lag model:\", e)\n\ntry:\n    # Fit the ML_Error model\n    model_error = ML_Error(y, X, w=w)\n    print(\"ML_Error Model Summary:\")\n    print(model_error.summary)\nexcept Exception as e:\n    print(\"Error in ML_Error model:\", e)\n\n# Check the columns after renaming\nprint(merged_gdf.columns)\n\n# Ensure the CRS is set\nif merged_gdf.crs is None:\n    merged_gdf.crs = 'EPSG:4326'  # or the appropriate EPSG code for your CRS\n\n# Save the updated GeoDataFrame to a shapefile\nmerged_gdf.to_file('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/merged_gdf.shp')\n# Wait so, it used to be saved to updated_merged_gdf, but then bec of the problem below:     I just changed the code to save it to merged_gdf, now I don't need the below code\n# Problem: Now I have a merged_gdf and updated_merged_gdf file\n\n# Checking if they are the same or not\n# Make sure to use the correct path and variable names\n#updated_merged_gdf = gpd.read_file('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/updated_merged_gdf.shp')\n# Rename columns in updated_merged_gdf to match merged_gdf\n#updated_merged_gdf = updated_merged_gdf.rename(columns={\n #   'County Nam': 'County Name',\n #  '% Change i': '% Change in Mortality Rate, 1980-2014'\n#})\n\n# Check columns after renaming\n#print(\"Updated columns in updated_merged_gdf:\")\n#print(updated_merged_gdf.columns)\n\n# Check if the indexes are the same\n#print(\"Indexes in merged_gdf:\")\n#print(merged_gdf.index)\n\n#print(\"Indexes in updated_merged_gdf:\")\n#print(updated_merged_gdf.index)\n\n# Check if the columns are the same\n#print(\"Columns in merged_gdf:\")\n#print(merged_gdf.columns)\n\n#print(\"Columns in updated_merged_gdf:\")\n#print(updated_merged_gdf.columns)\n\n# Check if the DataFrames are identical\n#are_equal = merged_gdf.equals(updated_merged_gdf)\n#print(\"Are merged_gdf and updated_merged_gdf identical?\")\n#print(are_equal)\n\n# They are the same\n# Will get rid of updated_merged_df then\n\n#del updated_merged_gdf\n\n\nimport libpysal as ps\nfrom mgwr.sel_bw import Sel_BW\n#import mgwr \n#from mgwr.sel import Sel_BW\n\n#import mgwr \n\n#try:\n#    from mgwr.sel import Sel_BW\n #   print(\"Sel_BW imported successfully!\")\n#except ImportError as e:\n #   print(f\"ImportError: {e}\")\n\n\n#coords = list(zip(gdf.geometry.centroid.x, gdf.#geometry.centroid.y))\n#y = gdf[['Mortality']].values\n#X = gdf[['PM2.5', 'Ozone']].values\n\n# Bandwidth selection\n\n#sel_bw = Sel_BW(coords, y, X,w=w)\n#bw = sel_bw.search()\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "k00tfg",
  "name" : "1. Data Prep",
  "description" : null,
  "code" : "import pandas as pd\nimport geopandas as gpd\n\n# Read CDC lung disease data\nlung_disease_df = pd.read_csv('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/CSV og data files/lung_disease_data.csv')\n\n# Read EPA air quality data for PM2.5 and ozone\npm25_df = pd.read_csv('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/Ozone and PM2.5 Data/combined_pm25_data.csv', low_memory=False)\nozone_df = pd.read_csv('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/Ozone and PM2.5 Data/combined_ozone_data.csv', low_memory=False)\n\n# Read county shapefile for spatial analysis\ncounties_gdf = gpd.read_file('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/County data/county_shapefile.shp')\n\n# Convert date column to datetime\npm25_df['Date Local'] = pd.to_datetime(pm25_df['Date Local'], format='%Y-%m-%d')\nozone_df['Date Local'] = pd.to_datetime(ozone_df['Date Local'], format='%Y-%m-%d')\n\n# Extract year from date\npm25_df['year'] = pm25_df['Date Local'].dt.year\nozone_df['year'] = ozone_df['Date Local'].dt.year\n\n# Get year from PM2.5\n\n# Convert 'Date Local' to datetime and extract year\npm25_df['Date Local'] = pd.to_datetime(pm25_df['Date Local'], format='%Y-%m-%d')\npm25_df['year'] = pm25_df['Date Local'].dt.year\n\n# Rename 'Sample Measurement' to 'PM2.5'\npm25_df.rename(columns={'Arithmetic Mean': 'PM2.5'}, inplace=True)\n\n# Verify the DataFrame\nprint(pm25_df.head())\nprint(pm25_df.columns)\n\n# Convert 'Date Local' to datetime and extract year\nozone_df['Date Local'] = pd.to_datetime(ozone_df['Date Local'], format='%m/%d/%y')\nozone_df['year'] = ozone_df['Date Local'].dt.year\n\n# Rename 'Sample Measurement' to 'Ozone'\nozone_df.rename(columns={'Arithmetic Mean': 'Ozone'}, inplace=True)\n\n# Verify the DataFrame\nprint(ozone_df.head())\nprint(ozone_df.columns)\n\n# Ensure 'County Name' and 'year' exist\nprint(pm25_df[['County Name', 'year']].head())\nprint(ozone_df[['County Name', 'year']].head())\n\n# Group by county and year to calculate annual averages\npm25_annual = pm25_df.groupby(['County Name', 'year'])['PM2.5'].mean().reset_index()\nozone_annual = ozone_df.groupby(['County Name', 'year'])['Ozone'].mean().reset_index()\n\n# Get latitude and longitude for each county-year pair\nlat_lon_pm25 = pm25_df.groupby(['County Name', 'year']).agg({\n    'Latitude': 'first',\n    'Longitude': 'first'\n}).reset_index()\n\nlat_lon_ozone = ozone_df.groupby(['County Name', 'year']).agg({\n    'Latitude': 'first',\n    'Longitude': 'first'\n}).reset_index()\n\n# Merge latitude and longitude with PM2.5 and Ozone averages\npm25_annual = pd.merge(pm25_annual, lat_lon_pm25, on=['County Name', 'year'])\nozone_annual = pd.merge(ozone_annual, lat_lon_ozone, on=['County Name', 'year'])\n# this worked but merging had error described below\n\n# Problems to fix: the lung disease dataframe columns are unnamed, the PM and ozone dataframe columns have 'County Name' while the lung disease doesn't\n\n# The header of the Lung disease data is 'Interstitial lung...' not the column name so...\n# Reload the lung disease dataframe with the correct header\nlung_disease_df = pd.read_csv('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/CSV og data files/lung_disease_data.csv', header=1)\n\n\n# Problem: lung disease data does not have 'County Name'\n# Rename columns to be consistent\nlung_disease_df.rename(columns={'Location': 'County Name'}, inplace=True)\n\n# Verify the columns, all should have 'County Name' and 'year'\nprint(\"PM2.5 DataFrame columns after renaming:\")\nprint(pm25_df.columns)\nprint(\"Ozone DataFrame columns after renaming:\")\nprint(ozone_df.columns)\nprint(\"lung_disease DataFrame columns after renaming:\")\nprint(lung_disease_df.columns)\n\n# problem is 'year'\n# extract the year from the columns, \"'Mortality Rate, 1980*', 'Mortality Rate, 1985*'\"\n# Melt the DataFrame\n\n# Use the melt function to transform the DataFrame from wide to long format. This will convert the year-specific columns into rows.\nlung_disease_long = lung_disease_df.melt(\n    id_vars=['County Name', 'FIPS', '% Change in Mortality Rate, 1980-2014'],\n    var_name='year',\n    value_name='Mortality Rate'\n)\n\n# Extract the Year\n# Extract the year from the 'Year' column using string operations and convert it to an integer.\nlung_disease_long['year'] = lung_disease_long['year'].str.extract(r'(\\d{4})').astype(int)\n# r'(\\d{4})' is a regex pattern that matches and captures a four-digit year.\n\n# test again and see what it looks like now, make sure to print the long version (changed version)\nprint(lung_disease_long)\n\n# Ensure that the year column exists in all DataFrames used in the merge operation. \nprint(\"Lung Disease DataFrame columns:\")\nprint(lung_disease_long.columns)\n\nprint(\"PM2.5 Annual DataFrame columns:\")\nprint(pm25_annual.columns)\n\nprint(\"Ozone Annual DataFrame columns:\")\nprint(ozone_annual.columns)\n\n# Convert year columns to integer type\nlung_disease_long['year'] = lung_disease_long['year'].astype(int)\npm25_annual['year'] = pm25_annual['year'].astype(int)\nozone_annual['year'] = ozone_annual['year'].astype(int)\n\n# Tried merging but the merged_df file contained no data\n# Problem: Lung disease data doesn't have latitude and longitutde like PM2.5 and ozone data\n# County Names are different\n\n# Check unique values in the County Name and year columns\nprint(lung_disease_long[['County Name', 'year']].drop_duplicates().head())\nprint(pm25_annual[['County Name', 'year']].drop_duplicates().head())\nprint(ozone_annual[['County Name', 'year']].drop_duplicates().head())\n\n# Define a function to standardize county names\ndef clean_county_name(name):\n    return name.strip().lower()\n\n# Apply the function to each DataFrame\nlung_disease_long['County Name'] = lung_disease_long['County Name'].apply(clean_county_name)\npm25_annual['County Name'] = pm25_annual['County Name'].apply(clean_county_name)\nozone_annual['County Name'] = ozone_annual['County Name'].apply(clean_county_name)\n\n\n# Remove 'county' from the County Name in the lung disease dataset\nlung_disease_long['County Name'] = lung_disease_long['County Name'].str.replace(' county', '', case=False)\n\n# Assuming the state and county columns are named 'State' and 'County Name'\n# Adjust column names if they are different in your datasets\n\n# Remove any extra spaces and clean up\npm25_annual['County Name'] = pm25_annual['County Name'].str.strip()\nozone_annual['County Name'] = ozone_annual['County Name'].str.strip()\n\n# Verify the new format\nprint(pm25_annual[['County Name', 'year']].drop_duplicates().sort_values(['County Name', 'year']))\nprint(ozone_annual[['County Name', 'year']].drop_duplicates().sort_values(['County Name', 'year']))\n\n\nprint(pm25_annual.head)\nprint(ozone_annual.head)\n\n# Convert county names to lowercase to standardize\npm25_annual['County Name'] = pm25_annual['County Name'].str.lower()\nozone_annual['County Name'] = ozone_annual['County Name'].str.lower()\nlung_disease_long['County Name'] = lung_disease_long['County Name'].str.lower()\n\n# Find common counties\ncommon_counties = set(pm25_annual['County Name']).intersection(set(ozone_annual['County Name']))\n\n# Filter DataFrames by common counties\npm25_common = pm25_annual[pm25_annual['County Name'].isin(common_counties)]\nozone_common = ozone_annual[ozone_annual['County Name'].isin(common_counties)]\nlung_disease_common = lung_disease_long[lung_disease_long['County Name'].isin(common_counties)]\n\n# Print results\nprint(\"PM2.5 DataFrame with common counties:\\n\", pm25_common)\nprint(\"Ozone DataFrame with common counties:\\n\", ozone_common)\nprint(\"Lung Disease DataFrame with common counties:\\n\", ozone_common)\nprint(f\"Number of common counties: {len(common_counties)}\")\n\n# Fixed county problem\n# Now I need to find common years\n\n# Find common years across the datasets\ncommon_years = set(pm25_common['year']).intersection(set(ozone_common['year'])).intersection(set(lung_disease_common['year']))\n\n# Filter DataFrames by common years\npm25_common = pm25_common[pm25_common['year'].isin(common_years)]\nozone_common = ozone_common[ozone_common['year'].isin(common_years)]\nlung_disease_common = lung_disease_common[lung_disease_common['year'].isin(common_years)]\n\n# Verify the common counties and years\nprint(f\"Number of common counties: {len(common_counties)}\")\nprint(f\"Number of common years: {len(common_years)}\")\n\n# Check unique years in each DataFrame\nprint(\"Unique years in PM2.5 data:\", pm25_common['year'].unique())\nprint(\"Unique years in Ozone data:\", ozone_common['year'].unique())\nprint(\"Unique years in Lung Disease data:\", lung_disease_common['year'].unique())\n\nprint(\"Number of rows in PM2.5 data before filtering:\", len(pm25_annual))\nprint(\"Number of rows in Ozone data before filtering:\", len(ozone_annual))\nprint(\"Number of rows in Lung Disease data before filtering:\", len(lung_disease_long))\n\n# Find common counties again and print them\ncommon_counties = set(pm25_annual['County Name']).intersection(set(ozone_annual['County Name']))\nprint(f\"Number of common counties: {len(common_counties)}\")\nprint(f\"Common counties: {common_counties}\")\n\n# Filter DataFrames by common counties and print their lengths\npm25_common = pm25_annual[pm25_annual['County Name'].isin(common_counties)]\nozone_common = ozone_annual[ozone_annual['County Name'].isin(common_counties)]\nlung_disease_common = lung_disease_long[lung_disease_long['County Name'].isin(common_counties)]\n\nprint(\"Number of rows in PM2.5 data after filtering:\", len(pm25_common))\nprint(\"Number of rows in Ozone data after filtering:\", len(ozone_common))\nprint(\"Number of rows in Lung Disease data after filtering:\", len(lung_disease_common))\n\nprint(\"Unique years in PM2.5 data before filtering:\", pm25_annual['year'].unique())\nprint(\"Unique years in Ozone data before filtering:\", ozone_annual['year'].unique())\nprint(\"Unique years in Lung Disease data before filtering:\", lung_disease_long['year'].unique())\n\n# Now county name and year columns should be fixed\n# Merge lung disease data with PM2.5 and ozone data\nmerged_df = pd.merge(lung_disease_long, pm25_annual, on=['County Name', 'year'])\nmerged_df = pd.merge(merged_df, ozone_annual, on=['County Name', 'year'])\n\nprint(merged_df.head)\n\nmerged_df.to_csv('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/merged_df.csv', index=False)\n\n\n# Find rows where Latitude_x and Latitude_y differ\ndiscrepancies = merged_df[merged_df['Latitude_x'] != merged_df['Latitude_y']]\n\n# Display the rows with discrepancies\nprint(discrepancies[['County Name', 'year', 'Latitude_x', 'Latitude_y']])\n\n# Drop the incorrect columns and rename the correct ones\nmerged_df = merged_df.drop(columns=['Latitude_y', 'Longitude_y'])\nmerged_df = merged_df.rename(columns={'Latitude_x': 'Latitude', 'Longitude_x': 'Longitude'})\n\n# Recreate the Geometry Column - Once you have cleaned up the latitude and longitude columns:\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n# Create the geometry column using the cleaned latitude and longitude columns\nmerged_df['geometry'] = merged_df.apply(lambda row: Point(row['Longitude'], row['Latitude']), axis=1)\n\n# Convert to GeoDataFrame\ngdf = gpd.GeoDataFrame(merged_df, geometry='geometry')\n\n# Optionally, set the coordinate reference system (CRS) if you know it\n# Example: gdf.set_crs(epsg=4326, inplace=True) # WGS84 CRS\n\n# Save the GeoDataFrame to a shapefile or other format if needed\ngdf.to_file('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/merged_geodataframe.shp')\n\n# Save the updated DataFrame to CSV\nmerged_df.to_csv('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/merged_df.csv', index=False)\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "adxzxk",
  "name" : "6. Geographically Weighted Regression (GWR)",
  "description" : null,
  "code" : "import numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom mgwr.gwr import GWR\nfrom mgwr.sel_bw import Sel_BW\nimport os\nimport libpysal as lp\n\nimport geopandas as gpd\n\nimport geopandas as gpd\n# Load the shapefiles\ngdf_updated = gpd.read_file('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/updated_geodataframe/updated_geodataframe.shp')\ngdf_merged = gpd.read_file('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/merged_gdf/merged_gdf.shp')\n\nimport pandas as pd\n\n# Load your GeoDataFrame or DataFrame\n# gdf = gpd.read_file('your_shapefile.shp')  # Example for GeoDataFrame\ngdf_merged['Mortality'] = gdf_merged['Mortality'].astype(str)\n\n# Extract the main value (before the parentheses)\ngdf_merged['Mortality'] = gdf_merged['Mortality'].str.extract(r'([0-9.]+)')\n\n# Convert the extracted value to float\ngdf_merged['Mortality'] = gdf_merged['Mortality'].astype(float)\n\n# Add an ID column with sequential integers\ngdf_merged['ID'] = range(1, len(gdf_merged) + 1)\n\n\n# Display the DataFrame to check the result\nprint(gdf_merged[['Mortality']].head())\n\n\nprint(\"Updated GeoDataFrame columns:\")\nprint(gdf_updated.columns)\n\nprint(\"Merged GeoDataFrame columns:\")\nprint(gdf_merged.columns)\n\n# Rename a column\ngdf_updated = gdf_updated.rename(columns={'County Nam': 'County Name'})\ngdf_merged = gdf_merged.rename(columns={'County Nam': 'County Name'})\n\n# Merge on key columns to identify unique rows\nkey_columns = ['County Name', 'FIPS', 'year']  # Adjust according to your data\n\n# Ensure both GeoDataFrames have the same columns for comparison\nunique_in_updated = gdf_updated.merge(gdf_merged, on=key_columns, how='left', indicator=True).query('_merge == \"left_only\"')\nprint(\"Rows in updated_geodataframe but not in merged_gdf:\")\nprint(unique_in_updated)\n\n\n# Load the shapefile into a GeoDataFrame\ngdf = gpd.read_file('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/merged_geodataframe/merged_geodataframe.shp')\nprint(gdf.head())\n\n# Rename the columns to fix any typos and remove unwanted suffixes\ngdf = gdf.rename(columns={'County Nam': 'County Name', '% Change i': '% Change in Mortality Rate, 1980-2014', 'geometry_x': 'geometry'})\nprint(gdf.head())\n\ncoords = list(zip(gdf.geometry.centroid.x, gdf.geometry.centroid.y))\ny = gdf[['Mortality']].values\nX = gdf[['PM2.5', 'Ozone']].values\n\n# Create the spatial weights matrix\nw = lp.weights.Queen.from_dataframe(gdf)\nw.transform = 'r'\n\n# Initialize the bandwidth selection object\nsel_bw = Sel_BW(coords, y, X)\n\ntry:\n    bw = sel_bw.search()\n    print(\"Bandwidth selection successful. Bandwidth:\", bw)\n    \n    # Proceed with GWR model fitting\n    gwr_model = GWR(coords, y, X, bw).fit()\n    print(gwr_model.summary())\nexcept Exception as e:\n    print(\"Error in bandwidth selection:\", e)\n    # Handle or log the exception as needed\n\n\n\n\n\n\n# Assuming gdf is your GeoDataFrame with cleaned data\ngdf_merged.to_file('/Users/icce_icecweam7/gw-workspace/S6wTraiideDo/merged_gdf/merged_gdf.shp', driver='ESRI Shapefile')\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
}]
